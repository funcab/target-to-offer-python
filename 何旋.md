# __何旋__  :raising_hand:

:phone: __16620487527__   &emsp;&emsp;&ensp;:e-mail: __hexuanr@foxmail.com__ 

:computer: [Github账号链接](https://github.com/funcab "Github账号链接")&emsp;&emsp;&ensp;:computer: [个人博客链接](https://funcab.github.io/ "个人博客链接")

:birthday: 1997.10.20&emsp;&emsp;&emsp;&emsp;&ensp;:house_with_garden: 北京市 朝阳区 西直河

### __求职意向__ :heartbeat:

:point_right:  __Python后端开发工程师__

### __教育背景__ :mortar_board:

<table>     
<tr>         
<td>
<center>华南理工大学</center>         
</td> 
<td>
<center>985 211</center>         
</td>  
<td>
<center>学士</center>         
</td>
<td>
<center>计算机科学与工程学院</center>         
</td>
<td>
<center>计算机科学与技术</center>         
</td>
<td>
<center>2013.09-2017.07</center>         
</td>
</tr> 
</table>

### __工作背景__ :memo:

<table>     
<tr>         
<td>
<center>中兴软创（经阿里巴巴收购更名蓝鲸科技）</center>
</td>   
<td>
<center>大数据开发工程师</center>         
</td>
<td>
<center>2017.07-2018.07</center>       
</td>
</tr> 
</table>

个人意愿辞职，出于兴趣想专注于Python后端开发的工作 

### __专业技能__ :books:

:point_right: 熟悉**Python**语言，了解C++、Shell语言

:point_right: 熟悉**Django**框架，了解Flask

:point_right: 熟悉常用的**数据结构和算法**，熟悉**计算机网络**、软件工程设计等计算机基础学科

:point_right: 熟悉**Mysql**，**Redis**数据库

:point_right: 熟悉基本的**Linux**操作命令

:point_right: 熟悉大数据**Hadoop分布式集群**组件**Hive**、**Spark**，了解Hbase、Kafka

:point_right: 曾多次参与项目，能与组员较好交流，**责任心**强，富有上进心，**自学能力**强

### __项目经历__ :computer:

Python Web：搭建技术**资源分享网站** | Linux+Django+MySQL+uWSGI+Nginx |  2018.02-2018.03

* 项目描述：用户**注册**，浏览或**添加**感兴趣的分类，在**分类**下添加相关**页面**，具有**上传下载**功能，根据浏览次数和点赞次数生成**热门**分类列表。
* 负责内容：

   :point_right: 在**Linux**环境下使用**Django**框架利用**MTV**模式实现**业务逻辑**和前后端交互
   
   :point_right: 采用反向代理服务器**Nginx**（静态解析）+**uWSGI**（动态解析）提高效率
   
   :point_right:  使用**Mysql**关系型数据库**Innodb**引擎获取更高的**增删查改**性能和并发性
   
   :point_right:  利用**装饰器**设置用户访问权限，用**Cookie**获取注册用户当日网站访问次数

大数据：**移动**互联网南方基地有数**数据仓库**项目 | Hadoop+Hive+Spark+Shell+ETL | 2017.7-2017.10
* 针对移动互联网的139邮箱、统一认证系统、和飞信、MM商城等业务通过每**日增量**或**全量**接口数据统计**日活/月活**数据传送至**报表**系统，**每日/每月**定时更新
* 负责内容：

   :point_right: 负责**统一认证业务报表**，基于**几十台**机器的**Hadoop集群**，数据量每日**TB**级
   
   :point_right: 调用**MapReduce**、Spark引擎，通过**HDFS**分布式文件系统，**分层**shell脚本编写，创建**Hive**数据仓库，并针对数据倾斜问题进行了MapReduce调优
   
   :point_right: 配置**ETL**调度，实现各层脚本**每日/月定时**按表**依赖**顺序执行，经清洗、转换、加工跑出报表数据，并可随时追踪**日志**
   
   :point_right: 通过和和客户**沟通反馈**，核对数据量，业务口径，分析并解决问题，修改取数细节




